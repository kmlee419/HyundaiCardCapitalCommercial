{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx7yp860pLh-"
   },
   "source": [
    "# Part 1 : Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sqhT2HCpLiE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9PTa1tCpLiG"
   },
   "source": [
    "## 1.1 Data\n",
    "\n",
    "* In this lecture, we use the SMS Spam Collection Data Set from UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). \n",
    "    * A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site.\n",
    "    * A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-YgEplBpLiH"
   },
   "outputs": [],
   "source": [
    "#df_sms = pd.read_csv('./SMS_Spam.tsv', sep='\\t')\n",
    "\n",
    "import io\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "df_sms = pd.read_csv(io.StringIO(uploaded['SMS_Spam.tsv'].decode('utf-8')), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyBMO4CmpLiH"
   },
   "outputs": [],
   "source": [
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNypc4CMpLiI"
   },
   "source": [
    "## 1.2 Exploratory Data Analysis\n",
    "* First, how many messages the data have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRlxbKxdpLiI"
   },
   "outputs": [],
   "source": [
    "len(df_sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEu3tgrOpLiJ"
   },
   "source": [
    "* Then, now, how many spams and hams each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-KZXF-ipLiJ"
   },
   "outputs": [],
   "source": [
    "df_sms['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLwfUERSpLiK"
   },
   "source": [
    "* Now, let's apply lengths of each message and create a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkJ0MR3WpLiK"
   },
   "outputs": [],
   "source": [
    "df_sms['length'] = df_sms['message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7xBEUllpLiL"
   },
   "outputs": [],
   "source": [
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ufnb5kjpLiL"
   },
   "source": [
    "* How are the lengths of messages distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lljkTBVpLiL"
   },
   "outputs": [],
   "source": [
    "sns.displot(df_sms['length'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYElnLQhpLiL"
   },
   "source": [
    "* Are there any differences of the distribution of spam and ham messages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HsjV3O1QpLiM"
   },
   "outputs": [],
   "source": [
    "df_spam = df_sms[df_sms['label']=='spam'].reset_index(drop=True)\n",
    "df_ham = df_sms[df_sms['label']=='ham'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4CdTSw0pLiM"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.displot(df_spam['length'], color='red')\n",
    "sns.displot(df_ham['length'], color='blue')\n",
    "plt.legend(labels=['spam','ham'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-T2k2S7wpLiM"
   },
   "source": [
    "## 1.3 Text preprocessing\n",
    "* For analyzing texts, we need to split each message into individual words.\n",
    "* Let's remove punctuations first.\n",
    "    * Python's built-in library **string** would provide a quick and convenient way of removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7NWFFnYpLiN"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOcz6L0apLiN"
   },
   "source": [
    "* Check characters whether they are punctuations or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAw4FtlNpLiN"
   },
   "outputs": [],
   "source": [
    "sample = \"Hello! This is Hyundai Card / Capital / Commercial - KAIST Ditigal Finance course\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59c_YQEppLiO"
   },
   "outputs": [],
   "source": [
    "sample_nopunc = []\n",
    "for char in sample:\n",
    "    if char not in string.punctuation:\n",
    "        sample_nopunc.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "395NE3cspLiO"
   },
   "outputs": [],
   "source": [
    "sample_nopunc = \"\".join(sample_nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYI03YcspLiO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox264rEBpLiO"
   },
   "source": [
    "* Now, it's a step to remove stopwords. The NLTK library is a kind of stardard library for processing texts in Python (https://www.nltk.org/).\n",
    "* The NLTK library provide a list of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h18JaLsmpLiP"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHryyOWxpLiP"
   },
   "source": [
    "* We can specify a language for stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhM42SFnpLiP"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKoy4gynpLiQ"
   },
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Zbr0-OPpLiQ"
   },
   "source": [
    "* Split the message and remove stopwords according to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJljrX7_pLiQ"
   },
   "outputs": [],
   "source": [
    "sample_nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRw34IlHpLiR"
   },
   "outputs": [],
   "source": [
    "sample_nopunc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_0OSCI8pLiR"
   },
   "outputs": [],
   "source": [
    "remove_stopwords = []\n",
    "for word in sample_nopunc.split():\n",
    "    if word.lower() not in stopwords.words('english'):\n",
    "        remove_stopwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e53YMRuHpLiS"
   },
   "outputs": [],
   "source": [
    "remove_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_t3E456vpLiS"
   },
   "source": [
    "* When you make a function for this, it would be more useful to apply it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gS4H9ZHzpLiS"
   },
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    \n",
    "    # remove punctuation\n",
    "    nopunc = []\n",
    "    for char in text:\n",
    "        if char not in string.punctuation:\n",
    "            nopunc.append(char)\n",
    "            \n",
    "    nopunc = \"\".join(nopunc)\n",
    "    \n",
    "    # remove stopwords\n",
    "    remove_stop = []\n",
    "    for word in nopunc.split():\n",
    "        if word.lower() not in stopwords.words('english'):\n",
    "            remove_stop.append(word)\n",
    "            \n",
    "    # remove words less than three characters\n",
    "    tokens = []\n",
    "    for word in remove_stop:\n",
    "        if len(word) >= 3:\n",
    "            tokens.append(word)\n",
    "            \n",
    "    #tokens = \" \".join(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKZrKiispLiT"
   },
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iT043QLEpLiT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preprocessing(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98mBdQgFpLiU"
   },
   "source": [
    "* You can apply the preprocessing function to whole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1pPlgmRpLiU"
   },
   "outputs": [],
   "source": [
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GW6jwiWFpLiU"
   },
   "outputs": [],
   "source": [
    "df_sms['message'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic_knuwOpLiV"
   },
   "source": [
    "## 1.4 Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lw0NxD2pLiV"
   },
   "outputs": [],
   "source": [
    "clean_spam = df_spam['message'].apply(preprocessing)\n",
    "clean_ham = df_ham['message'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47JSixSKpLiV"
   },
   "source": [
    "* First, let's merge whole values of each dataframe into one list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhHZuJQ4pLiV"
   },
   "outputs": [],
   "source": [
    "whole_spam = []\n",
    "for line in clean_spam.tolist():\n",
    "    whole_spam += line\n",
    "    \n",
    "whole_ham = []\n",
    "for line in clean_ham.tolist():\n",
    "    whole_ham += line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAZ-_kMspLiW"
   },
   "source": [
    "* The **Text** class in **NLTK** library provide some useful methods to text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_qZDLhfpLiW"
   },
   "outputs": [],
   "source": [
    "from nltk import Text\n",
    "\n",
    "ham_text = Text(whole_ham)\n",
    "spam_text = Text(whole_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aouAFbmvpLiW"
   },
   "source": [
    "* The **vocab** method in the **Text** class can extract the frequency of usage for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRVjY9mxpLiW"
   },
   "outputs": [],
   "source": [
    "freqDist_ham = ham_text.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55V6cBoRpLiX"
   },
   "outputs": [],
   "source": [
    "freqDist_ham.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zltNPoSpLiX"
   },
   "source": [
    "* How about spam messages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TMs9Fv5pLiX"
   },
   "outputs": [],
   "source": [
    "freqDist_spam = spam_text.vocab()\n",
    "freqDist_spam.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKQBB7Y_pLiX"
   },
   "source": [
    "* You can plot the distribution of each token by the **plot** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmmLhsOnpLiY"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "ham_text.plot(30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBIjJFmGpLiY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrp7LgcMpLiY"
   },
   "source": [
    "* We can also use the **wordcloud** package for visualization. \n",
    "* You can download the package by `conda install -c conda-forge wordcloud`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBtbavc8pLiY"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "wc_ham = WordCloud(width=1000, height=600, background_color=\"black\", random_state=0)\n",
    "plt.imshow(wc_ham.generate_from_frequencies(freqDist_ham))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MHhyYgcpLiZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAhM1Xn-pLiZ"
   },
   "source": [
    "# Part 2 : Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJY2WB9npLiZ"
   },
   "source": [
    "* Recommendation system is a sort of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. They are primarily used in commercial applications (https://en.wikipedia.org/wiki/Recommender_system)\n",
    "* There are two common types of recommender systems:\n",
    "    * **Content-Based Filtering** focus on the attributes of the items and give you recommendations based on the similarity between them.\n",
    "    \n",
    "    * **Collaborative Filtering** produces recommendations based on the user's attitude (activity) to items.\n",
    "\n",
    "\n",
    "* Movie recommendation is one of the first step to start learning recommendation systems.\n",
    "* MovieLens dataset is a famous one for learning to build the recommendation systems.\n",
    "    * https://grouplens.org/datasets/movielens/\n",
    "    * https://kaggle.com/grouplens/movielens-20m-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movies = pd.read_csv('./movies.csv')\n",
    "\n",
    "uploaded = files.upload()\n",
    "movies = pd.read_csv(io.StringIO(uploaded['movies.csv'].decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's produce a content-based filtering based on genre similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "genre_vec = vectorizer.fit_transform(movies['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = cosine_similarity(genre_vec, genre_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_sim = pd.DataFrame(index=movies['title'], columns=movies['title'], data=sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, let's import one more dataset \"ratings.csv\", and produce collaborative filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings = pd.read_csv('./ratings.csv')\n",
    "\n",
    "uploaded = files.upload()\n",
    "ratings = pd.read_csv(io.StringIO(uploaded['ratings.csv'].decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's merge those two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which movie has the highest user ratings on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which movies received the most ratings from users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's combine of those two results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings = pd.DataFrame(df_movies.groupby('title')['rating'].mean())\n",
    "movie_ratings['numbers'] = pd.DataFrame(df_movies.groupby('title')['rating'].count())\n",
    "movie_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, reshape the dataframe with using pivot_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fill the NaN values to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's take two examples of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix = user_movie_matrix['Matrix, The (1999)']\n",
    "Matrix.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Terminator = user_movie_matrix['Terminator 2: Judgment Day (1991)']\n",
    "Terminator.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How similar with those two movies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which movie is the most similar with the \"Matrix, The (1999)\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix_corr = pd.DataFrame(user_movie_matrix.corrwith(Matrix), columns=['correl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix_corr.sort_values(by='correl', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, let's use one more metric for similarity - Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_user_matrix = df_movies.pivot_table(index='title', columns='userId', values='rating')\n",
    "movie_user_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_user_matrix.fillna(0, inplace=True)\n",
    "\n",
    "item_based_filter = cosine_similarity(movie_user_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_based_matrix = pd.DataFrame(index=movie_user_matrix.index, columns=movie_user_matrix.index, data=item_based_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAhM1Xn-pLiZ"
   },
   "source": [
    "# Part 3 : Cluster analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clustering is a Machine Learning technique that involves the grouping of data points.\n",
    "    * https://en.wikipedia.org/wiki/Cluster_analysis\n",
    "    * https://developers.google.com/machine-learning/clustering/clustering-algorithms\n",
    "* K-means clustering is one of the simplest and popular unsupervised machine learning algorithms that will attempt to group similar clusters together in data (https://en.wikipedia.org/wiki/K-means_clustering).\n",
    "* You can also check out further materials to learn.\n",
    "    * http://www.mit.edu/~9.54/fall14/slides/Class13.pdf\n",
    "    * https://www.youtube.com/watch?v=Ev8YbxPu_bQ\n",
    "    * https://www.youtube.com/watch?v=hDmNF9JG3lo\n",
    "    * https://www.coursera.org/learn/data-science-k-means-clustering-python#syllabus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's apply the k-means algorithm to the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('./samples.csv')\n",
    "\n",
    "uploaded = files.upload()\n",
    "data = pd.read_csv(io.StringIO(uploaded['samples.csv'].decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='V1', y='V2', data=data, kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Of course, the scikit-learn package provide model for k-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans_fit = kmeans.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How about applying a different K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How can we decide the best $K$ value? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = {}\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k).fit(data)\n",
    "    sse[k] = kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(sse.keys())\n",
    "y = list(sse.values())\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfogugXQW03R"
   },
   "source": [
    "* Now, let's take a look at the customer segmentation data,\n",
    "    * https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers = pd.read_csv('./Mall_Customers.csv')\n",
    "\n",
    "uploaded = files.upload()\n",
    "customers = pd.read_csv(io.StringIO(uploaded['Mall_Customers.csv'].decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsVBQeNGmOLD"
   },
   "source": [
    "* Let's apply clustering algorithm with two features, annual income and spending score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = 'Annual_Income', y = 'Spending_Score',  data = customers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = customers[['Annual_Income', 'Spending_Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans_fit = kmeans.fit(data)\n",
    "labels = kmeans_fit.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='Annual_Income', y='Spending_Score', hue='cluster',data=customers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = {}\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k).fit(data)\n",
    "    sse[k] = kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(sse.keys())\n",
    "y = list(sse.values())\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One more thing with saving the trained model and reload to predict.\n",
    "    * You can produce a kind of Web App with streamlit (https://streamlit.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "dump(kmeans, './Mall_Customers_clusters.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load('./Mall_Customers_clusters.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income = 80\n",
    "score = 20\n",
    "\n",
    "row = [income, score]\n",
    "feat_cols = data.columns\n",
    "\n",
    "df = pd.DataFrame([row], columns = feat_cols)\n",
    "features = pd.DataFrame(df, columns = feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "section3-python_applications.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
